# Влияние Твиттера на финансовые рынки

Данный репозиторий содержит мою итоговою проектную работу по курсу OTUS "**Machine Learning. Professional**" на тему "**Влияние Твиттера на финансовые рынки**".

Целью работы является проверка мифа о том, что твиты влиятельных людей могут влиять на фондовые рынки. В данной работе планируется:

- Попытаться кластеризовать твиты различными методами
- Попытаться сделать тематическое моделирование твитов
- Попытаться предсказать рост/падение актива по содержимому твита

В данном [README](README.md) содержится краткое описание проделанной работы с необходимыми вставками кода и графиков из соответствующих jupyter notebook-ов, ссылки на которые будут даны по тексту работы.

## Данные

Краеугольным камнем любого ML проекта являются данные. Для моего проекта мне потребовались данные о твитах и о биржевом курсе финансового актива.

### Твиты

В качестве людей, чьи аккаунты я использовал, я выбрал Илона Маска и Дональда Трампа, так как они являются активными пользователями твиттера и считается, что твиты Маска могут влиять на курсы криптовалют (таких как DOGE).

Изначально я собирался спарсить твиттер при помощи beautifulsoup4, но с этим планом возникло несколько проблем:

- Твиттер - это SPA, которое динамически подгружает содержимое страницы, поэтому просто спарсить html не получится ([пруф](https://stackoverflow.com/questions/68987825/problem-while-scraping-twitter-using-beautiful-soup)).
- С недавнего времени Твиттер заблокирован в России.
- Аккаунт Дональда Трампа заблокирован в Твиттере.

Хотя часть этих проблем решаема, я решил найти готовые наборы данных. На помощь, как это часто бывает, пришел Kaggle. Мною были найдены архивы твитов Илона Маска ([ссылка](https://www.kaggle.com/datasets/ayhmrba/elon-musk-tweets-2010-2021)) и Дональда Трампа ([ссылка](https://www.kaggle.com/datasets/headsortails/trump-twitter-archive)). Данные наборы содержат разные столбцы, среди которых нас будут интересовать такие общие столбцы как ID, дата и текст твита. Для простоты я взял твиты за период с 01.01.2019 по 31.12.2021.

### Биржевой курс

В качестве источника цены финансового актива мной был взят [Yahoo Finance](https://finance.yahoo.com/). Я выбрал следующие активы:

- Криптовалюта Bitcoin ([ссылка](https://finance.yahoo.com/quote/BTC-USD/history?p=BTC-USD))
- Криптовалюта Dogecoin ([ссылка](https://finance.yahoo.com/quote/DOGE-USD/history?p=DOGE-USD))
- Акции компании Tesla ([ссылка](https://finance.yahoo.com/quote/TSLA/history?p=TSLA))

## Предобработка данных

После загрузки данных их необходимо предобработать.

### Твиты

Как было сказано раньше, данные твитов Илона Маска и Дональда Трампа имели разный набор столбцов. Из них я выделил такие общие столбцы как ID, дата и текст твита.
Затем все тексты были специальным образом обработаны (удалены упоминания других пользователей, ссылки, служебные html символы, а также заменены все последовательности небуквенных символов на пробел), используя следующую функцию:

```python
def text_preprocessor(v: str) -> str:
    v = v.lower()
    v = re.sub(r'@[^\s]+', '', v)
    v = re.sub(r'https?://[^\s]+', '', v)
    v = re.sub(r'&\w+;', '', v)
    v = re.sub('\W+', ' ', v)
    v = v.strip()
    return v
```

После данной обработки пустые твиты были удалены (они содержали только ссылки, упоминания других пользователей и т.п.).

В результате получился набор данных из 8643 твитов Илона Маска и 18649 твитов Дональда Трампа. Данный набор содержит столбцы id, date, text и cleared_text.

Для ускорения дальнейшей работы для каждого твита были сгенерированы векторные представления при помощи [векторов из библиотеки FastText](https://fasttext.cc/docs/en/english-vectors.html) с весами TF-IDF и [трансформерной модели LaBSE](https://huggingface.co/cointegrated/LaBSE-en-ru).

Полный исходный код обработки представлен в notebook-ах [Tweets Preprocessing](preprocessing/Tweets%20Preprocessing.ipynb) и [Tweets Preprocessing Transformers](preprocessing/Tweets%20Preprocessing%20Transformers.ipynb).

### Биржевой курс

Данные, загруженные с Yahoo Finance, содержат столбцы Date (Дата торгов), Open (Цена на момент открытия), High (Максимальная цена), Low (Минимальная цена), Close (Цена на момент закрытия), Adj Close (Скорректированная цена на момент закрытия) и Volume (Объем торгов).

Единственной обработкой, примененной к данным данным, было заполнение пропусков в выходные дни значениями из ближайшего следующего рабочего дня (данное справедливо только к акциям/обычным валютам, так как биржа в выходные дни не работает).

Исходный код можно найти в notebook-е [Tweets Ticker Template](ticker%20prediction/Tweets%20Ticker%20Template.ipynb) в разделе **Data loading**

## Кластеризация

Следующим этапом была кластеризация. Я попытался при помощи различных методов генерации векторных представлений текстов (TF-IDF, FastText и LaBSE-en-ru) и различных методов кластеризации (KMeans, DBScan и иерархическая кластеризация) разделить твиты на интерпретируемые кластеры.

Для визуализации на двумерной плоскости использовались методы PCA и UMAP (стандартный и с метрикой косинусной близости).

В данной части работы будет приведена только часть примеров полученных кластеров, но, помимо этого, будут даны ссылки на соответствующие Jupyter Notebook-и.

### TF-IDF

Сначала я попробовал TF-IDF для генерации векторных представлений. Рассмотрим получившиеся результаты на примере Илона Маска.

Визуализация твитов на плоскости выглядит следующим образом:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

Видно, что если после PCA визуально выделяются два кластера, то после UMAP выделяется только один с некоторым количеством выбросов.

**[ССЫЛКИ НА НОТЕБУКИ]**

#### KMeans

Затем я попытался применить KMeans для кластеризации. Метод локтя и коэффициент силуэта показал, что число кластеров равное двум является оптимальным.

В результате получились следующие кластеры:

1. Кластер 1. Состоит из 8404 твитов и содержит почти все твиты Маска.
2. Кластер 2. Состоит из 241 твита, содержащих в себе слово 'yes'.

Топ-3 слов по встречаемости во втором кластере:

1. yes - 241 раза
2. haha - 3 раза
3. irony - 1 раз

Примеры таких твитов:

| id                  | text                                     | date       |
| ------------------- | ---------------------------------------- | ---------- |
| 1210777492027363328 | @TheAssassin_95 Yes, but it’s too secret | 2019-12-28 |
| 1208832682693541888 | @RenataKonkoly Yes 🤣♥️                  | 2019-12-22 |
| 1193555008001728512 | @xtrmmax @getjeda @Tesla Yes             | 2019-11-10 |

Видно, что это, скорее всего, ответы на вопросы.

На плоскости эти кластеры выглядят следующим образом:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

#### DBScan

Следующим методом был DBScan. Коэффициент силуэта показал, что оптимальным числом кластеров является 125 (1 для выбросов) для eps = 0.755. В результате получилось много небольших кластеров из односложных твитов и выбросы, которых было ~75% от общего числа твитов и куда попали более сложные твиты.

Рассмотрим один из этих кластеров (Состоит из 26 твитов).

Топ-3 слова:

1. coming - 26 раз
2. soon - 26 раз
3. yes - 2 раза

Примеры твитов:

| id                  | text                          | date       |
| ------------------- | ----------------------------- | ---------- |
| 1173323478264623112 | @TeslaTested Yes, coming soon | 2019-09-15 |
| 1183509770382135297 | @TeslaGong Coming v soon      | 2019-10-13 |
| 1112035721672482816 | @MoWo91 Coming soon!          | 2019-03-30 |

Визуализация на плоскости:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

#### Иерархическая кластеризация

Последним рассмотренным методом была иерархическая кластеризация (использовал AgglomerativeClustering из библиотеки sklearn). Используя метод силуэта, я получил, что 250 кластеров - это оптимальное число (большиЕ числа я не проверял :)). В результате получилось много маленьких, односложных, плохо интерпретируемых кластеров.

Их примерный состав:

```
Cluster   0.    24 tweets total. Top-3 words: [('fine', 8), ('shirt', 6), ('fix', 5)]
Cluster   1.    37 tweets total. Top-3 words: [('go', 16), ('live', 11), ('goes', 10)]
Cluster   2.    32 tweets total. Top-3 words: [('life', 29), ('multiplanetary', 17), ('light', 9)]
Cluster   3.    35 tweets total. Top-3 words: [('play', 14), ('mode', 13), ('11', 9)]
Cluster   4.   105 tweets total. Top-3 words: [('landing', 36), ('heat', 26), ('starship', 24)]
Cluster   5.    38 tweets total. Top-3 words: [('bar', 26), ('test', 23), ('pressure', 19)]
Cluster   6.  3490 tweets total. Top-3 words: [('tesla', 245), ('would', 151), ('high', 127)]
Cluster   7.    30 tweets total. Top-3 words: [('better', 23), ('slightly', 8), ('much', 7)]
Cluster   8.    20 tweets total. Top-3 words: [('tank', 17), ('liquid', 11), ('oxygen', 9)]
Cluster   9.    54 tweets total. Top-3 words: [('production', 50), ('hard', 19), ('easy', 14)]
Cluster  10.    27 tweets total. Top-3 words: [('really', 14), ('universe', 11), ('journey', 6)]
...
Cluster 239.    28 tweets total. Top-3 words: [('cool', 28), ('super', 2), ('way', 1)]
Cluster 240.    15 tweets total. Top-3 words: [('earth', 20), ('mars', 14), ('gravity', 4)]
Cluster 241.    14 tweets total. Top-3 words: [('mars', 14), ('real', 2), ('looking', 2)]
Cluster 242.    10 tweets total. Top-3 words: [('fsd', 16), ('computer', 15), ('tesla', 9)]
Cluster 243.     6 tweets total. Top-3 words: [('coming', 6), ('year', 2)]
Cluster 244.     9 tweets total. Top-3 words: [('moon', 10), ('going', 3), ('starship', 2)]
Cluster 245.    10 tweets total. Top-3 words: [('good', 10), ('yes', 1)]
Cluster 246.    12 tweets total. Top-3 words: [('summon', 13), ('smart', 9), ('tesla', 4)]
Cluster 247.    22 tweets total. Top-3 words: [('giga', 27), ('berlin', 23), ('tesla', 3)]
Cluster 248.     5 tweets total. Top-3 words: [('sucks', 5), ('twitter', 1), ('app', 1)]
Cluster 249.     6 tweets total. Top-3 words: [('norway', 6), ('sorry', 1), ('mission', 1)]
```

Визуализация на плоскости:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

### FastText

Далее я попробовал использовать векторы из библиотеки FastText для генерации векторных представлений. Представление твита получалось как средне взвешенное представление каждого слова в твите с весами, равными значению TF-IDF для каждого слова. Функция для генерации векторного представления выглядит следующим образом:

```python
def tweet_embedding(text: str) -> np.array:
    embedding = np.zeros(300) # векторное представление твита
    tfidf = 0 # сумма всех весов (tf-idf)

    text = text_preprocessor(text) # предобработка твита
    words_tfidf = tfidf_dict(text) # словарь со значениями tf-idf для слов в твите

    for word in text.split(' '):
        if word in word_embeddings and word in words_tfidf:
            word_tfidf = words_tfidf[word]
            tfidf += word_tfidf
            embedding += word_tfidf * word_embeddings[word]

    if tfidf:
        embedding /= tfidf

    return embedding
```

В качестве примеров опять рассмотрим твиты Илона Маска.

На плоскости они выглядят следующим образом:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

Визуально различим только один кластер с множеством выбросов.

**[ССЫЛКИ НА НОТЕБУКИ]**

#### KMeans

Используя KMeans удалось разделить данные на восемь классов:

| Кластер | Число элементов | Топ-3 слова (слова, количество вхождений)    |
| ------- | --------------- | -------------------------------------------- |
| 0       | 1667            | 'yeah', 69<br>'ok', 61<br>'like', 50         |
| 1       | 1618            | 'great', 114<br>'good', 112<br>'much', 91    |
| 2       | 4769            | 'tesla', 574<br>'high', 208<br>'would', 207  |
| 3       | 76              | 'sure', 76<br>'hope', 4<br>'ok', 1           |
| 4       | 78              | 'haha', 76<br>'yes', 3<br>'awesome', 3       |
| 5       | 221             | 'yes', 221                                   |
| 6       | 121             | 'true', 118<br>'haha', 24<br>'truth', 3      |
| 7       | 95              | 'exactly', 92<br>'precisely', 3<br>'yeah', 1 |

Видно, что из данных выделилось три больших кластеров, а также несколько простых, односложных кластеров.

На плоскости они выглядят следующим образом:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

#### DBScan

Используя DBScan и метод силуэта, получилось что оптимальным числом кластеров является либо один (один кластер и выбросы), либо ноль (только выбросы), так как им соответствовал одинаковый коэффициент силуэта.

Для интереса я выбрал число кластеров, равное двум и получил следующее:

| Кластер | Число элементов | Топ-3 слова (слова, количество вхождений)   |
| ------- | --------------- | ------------------------------------------- |
| -1      | 159             | 'seriously', 4<br>'touché', 3<br>'dance', 3 |
| 0       | 8451            | 'tesla', 661<br>'yes', 385<br>'great', 326  |
| 1       | 35              | 'absolutely', 35                            |

На плоскости они выглядят так:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

#### Иерархическая кластеризация

Используя иерархическую кластеризацию и метод силуэта, получилось шесть кластеров:

| Кластер | Число элементов | Топ-3 слова (слова, количество вхождений)   |
| ------- | --------------- | ------------------------------------------- |
| 0       | 6973            | 'tesla', 637<br>'great', 313<br>'good', 285 |
| 1       | 1202            | 'haha', 83<br>'ok', 55<br>'yeah', 47        |
| 2       | 87              | 'true', 87                                  |
| 3       | 221             | 'yes', 221                                  |
| 4       | 91              | 'exactly', 91                               |
| 5       | 71              | 'sure', 71                                  |

Визуализация на плоскости:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

### LaBSE-en-ru

В качестве последнего метода генерации векторных представлений я использовал трансформенную модель [LaBSE-en-ru](https://huggingface.co/cointegrated/LaBSE-en-ru), позволяющую генерировать векторные представления для целых предложений.

Векторные представления твитов Илона Маска выглядят следующим образом:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ МАСКА]**

А твиты Дональда Трампа следующим:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ ТРАМПА]**

При использовании PCA получились кластеры интересной формы.

В качестве примеров рассмотрим твиты Дональда Трампа.

**[ССЫЛКИ НА НОТЕБУКИ]**

#### KMeans

При помощи KMeans получилось три хорошо разделимых кластера:

| Кластер | Число элементов | Топ-10 слов (слова, количество вхождений)                                                                                                                                        |
| ------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0       | 3141            | 'thank', 726<br>'great', 629<br>'maga', 145<br>'america', 134<br>'big', 118<br>'news', 115<br>'see', 95<br>'p', 90<br>'congratulations', 88<br>'make', 78                        |
| 1       | 8914            | 'president', 1559<br>'trump', 1299<br>'democrats', 631<br>'biden', 579<br>'people', 420<br>'today', 407<br>'impeachment', 401<br>'american', 390<br>'new', 384<br>'america', 337 |
| 2       | 6594            | 'great', 1546<br>'people', 1042<br>'amp', 856<br>'president', 829<br>'democrats', 722<br>'country', 719<br>'news', 662<br>'big', 661<br>'fake', 660<br>'would', 646              |

Примеры твитов из нулевого кластера:

- Getting a little exercise this morning!
- Thank you Elise!
- THANK YOU OHIO! #VOTE
- 11 DAYS! #MAGA
- Thank you Megyn!

Кластеры на плоскости:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ ТРАМПА]**

#### DBScan

DBScan и метод силуэта показал, что оптимальным числом кластеров является один (один кластер и выбросы). Содержание о визуализацию кластеров опустим.

#### Иерархическая кластеризация

При помощи иерархической кластеризации удалось разделить данные на два класса: класс, где трамп благодарен чему-то, а также просит сделать Америку великой снова, и всё остальное.

| Кластер | Число элементов | Топ-3 слова (слова, количество вхождений)           |
| ------- | --------------- | --------------------------------------------------- |
| 0       | 16864           | 'president', 2417<br>'great', 2182<br>'trump', 1821 |
| 1       | 1785            | 'thank', 479<br>'great', 295<br>'maga', 74          |

Примеры твитов:

- Thank you Elise!
- As per your request, Joe...
- Thank you Megyn!
- 11 DAYS! #MAGA
- THANK YOU OHIO! #VOTE

Визуализация на плоскости:

**[СУПЕР КРУТАЯ КАРТИНКА С КЛАСТЕРАМИ ТРАМПА]**

## Тематическое моделирование

После кластеризации я занялся тематическим моделированием твитов. Мною били использованы алгоритмы NMF (Non-negative matrix factorization) и LDA (Latent Dirichlet allocation).

**[ССЫЛКА НА НОТЕБУКИ]**

Посмотрим на полученные для Дональда Трампа и Илона Маска топики.

### Дональд Трамп

Для Дональда Трампа, используя NMF с различными loss-функциями, удалось получить следующие 6 топиков:

**[Картинка с топиками Трампа]**

**[Картинка с топиками Трампа]**

Используя LDA:

**[Картинка с топиками Трампа]**

Получились следующие топики:

1. Общие слова
2. Американские президенты
3. Демократы
4. Выборы Трампа
5. Фейковые новости
6. Америка

Динамика изменения тем в его твитах:

**[Картинка с динамикой топиков Трампа]**

### Илон Маск

Для Илона Маска, используя NMF с различными loss-функциями, удалось получить следующие 6 топиков:

**[Картинка с топиками Маска]**

**[Картинка с топиками Маска]**

Используя LDA:

**[Картинка с топиками Маска]**

Получились следующие топики:

1. Общие слова
2. Еще общие слова
3. Tesla
4. Будущее
5. Производство в Tesla/SpaceX
6. Ответы "Да"

Динамика изменения тем в его твитах:

**[Картинка с динамикой топиков Илона]**

## Предсказание роста/падения акций/криптовалют

Последним этапом моей работы была попытка предсказания вырастит или упадет стоимость актива по твиту.

В качестве примера рассмотрим предсказание изменения стоимости Doge по твитам Илона Маска.

**[Очень много ссылок на нотебуки]**

### Обзор данных

Для обучения моделей использовался набор данных, содержащий обработанный текст твита и флаг того, что Doge вырос на следующий день (0 - не вырос, 1 - вырос), который я и пытался предсказать.

Посмотрим на моменты написания твитов на графике стоимости:

**[Супер крутая картинка стоимости доге с датами твитов]**

Из-за частоты написания твитов Илоном Маском, все превратилось в красный прямоугольник :(.

Посмотрим как часто Doge рос:

```python
tweets_df['ticker_grow_in_this_day'].mean() # 0.5205
```

Посмотрим какие слова чаще всего использовал Илон при росте, чем при падении:

1. block
2. irl
3. turned
4. allocation
5. direct
6. whether
7. cov2
8. action
9. basically
10. 80

А теперь слова, чаще используемые при падении, чем при росте:

1. insanely
2. 13
3. startups
4. opposite
5. summer
6. kids
7. anyway
8. nose
9. operational
10. site

### Baseline

Начнем прогноз с baseline модели. В качестве модели я использовал дискретную случайную величину со значениями 0, 1 с математическим ожиданием, совпадающим с ожиданием того, что Doge вырастет.

Генерация предсказаний выглядит следующим образом:

```python
y_baseline = np.int32(rnd.random(y_test.shape) <= y.mean())
```

Для данного предсказания получил следующее качество на тесте:

| Модель   | f1-score (0) | f1-score (1) | Accuracy |
| -------- | ------------ | ------------ | -------- |
| Baseline | 0.49         | 0.53         | 0.51     |

### Logistic regression

Следующей использованной моделью была логистическая регрессия с подбором силы регуляризации. В качестве входа я использовал TF-IDF представление твита.

Ее качество на тестовой выборке:

| Модель               | f1-score (0) | f1-score (1) | Accuracy |
| -------------------- | ------------ | ------------ | -------- |
| LogisticRegressionCV | 0.27         | 0.64         | 0.52     |

Использование логистической регрессии дает нам возможность при помощи ее коэффициентов оценить, как и какие слова влияют на курс Doge (в нашем случае, это скорее случайный набор чисел, чет что-то осмысленное, так как точность модели близка к случайной):

Топ-10 слов по положительному влиянию:

| Слово           | Коэффициент |
| --------------- | ----------- |
| ok              | 0.248964    |
| like            | 0.231684    |
| basically       | 0.226820    |
| think           | 0.219637    |
| thanks          | 0.189326    |
| congratulations | 0.184741    |
| sounds          | 0.158596    |
| work            | 0.156333    |
| much            | 0.155555    |
| fast            | 0.154979    |
| hope            | 0.142692    |
| space           | 0.136811    |
| indeed          | 0.134677    |
| higher          | 0.133793    |
| someone         | 0.120802    |

Топ-10 слов по отрицательному влиянию:

| Слово   | Коэффициент |
| ------- | ----------- |
| good    | -0.294189   |
| always  | -0.272107   |
| rocket  | -0.228024   |
| best    | -0.220713   |
| things  | -0.213014   |
| launch  | -0.193191   |
| day     | -0.182693   |
| agreed  | -0.178804   |
| ai      | -0.178778   |
| doge    | -0.175455   |
| make    | -0.175161   |
| try     | -0.172151   |
| weeks   | -0.164778   |
| landing | -0.164610   |
| yes     | -0.164062   |

### Random Forest

Последней моделью был случайный лес, использующий TF-IDF представление текста в качестве входа.

Его качество на тесте с подборкой гиперпараметров и без:

| Модель                              | f1-score (0) | f1-score (1) | Accuracy |
| ----------------------------------- | ------------ | ------------ | -------- |
| RandomForestClassifier              | 0.48         | 0.57         | 0.53     |
| RandomForestClassifier (GridSearch) | 0.10         | 0.68         | 0.52     |

Использование случайного леса, также позволяет оценить важность конкретного слова для предсказания:

| Слово     | Важность |
| --------- | -------- |
| good      | 0.024013 |
| rocket    | 0.014589 |
| always    | 0.014300 |
| best      | 0.012512 |
| basically | 0.011440 |
| months    | 0.010901 |
| next      | 0.010575 |
| things    | 0.010387 |
| day       | 0.010090 |
| like      | 0.009975 |

### Интерпретация результатов

Сводные результаты всех моделей:

| Модель                              | f1-score (0) | f1-score (1) | Accuracy |
| ----------------------------------- | ------------ | ------------ | -------- |
| Baseline                            | **0.49**     | 0.53         | 0.51     |
| LogisticRegressionCV                | 0.27         | 0.64         | 0.52     |
| RandomForestClassifier              | 0.48         | 0.57         | **0.53** |
| RandomForestClassifier (GridSearch) | 0.10         | **0.68**     | 0.52     |

Хотя по точности лучше всех себя показал стандартный случайный лес, из результатов видно, что модели недалеко ушли от случайного предсказания. Можно сделать вывод, что предсказывать стоимость акций по содержимому твитов не удастся:(.
